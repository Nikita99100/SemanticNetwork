{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepPavlovPract.ipynb","provenance":[],"mount_file_id":"1IZGDMNJl5-x2hLHvCaI6eP49t9cDVUgQ","authorship_tag":"ABX9TyMLvFthACqiCQ3DusBnKqAz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NPqoj0Hx2Ad-"},"source":["# **Морфологическая маркировка**"]},{"cell_type":"markdown","metadata":{"id":"xCJeLTv8oryo"},"source":["На данный момент у нас есть два типа моделей: модели на основе BERT (доступны только для русского языка) и символьные двунаправленные LSTM. Модель BERT включает в себя только плотный слой поверх BERT embedder. Планируется выпустить больше моделей на базе BERT в ближайшем будущем.\n","Большинство моделей соответствуют Heigold et al., 2017(Обширная эмпирическая оценка символьных морфологических тегов для 14 языков). Они также достигают высочайшего уровня производительности среди систем с открытым исходным кодом.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jLMokfAWmeDw"},"source":["Устанавливаем DeepPavlov"]},{"cell_type":"code","metadata":{"id":"MTwDy2gMmkwm","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1624602085931,"user_tz":-180,"elapsed":56307,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"856e5d1a-884e-4955-a02d-b0d2dda2a873"},"source":["!pip3 install deeppavlov"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting deeppavlov\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/87/e77ccc7de09f8c5c4a3d981ff6b1d3811d9978976a30bec9bdf50d667ebb/deeppavlov-0.15.0-py3-none-any.whl (907kB)\n","\u001b[K     |████████████████████████████████| 911kB 10.7MB/s \n","\u001b[?25hCollecting pymorphy2-dicts-ru\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 14.4MB/s \n","\u001b[?25hCollecting scikit-learn==0.21.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 24.2MB/s \n","\u001b[?25hCollecting uvicorn==0.11.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n","\u001b[?25hCollecting ruamel.yaml==0.15.100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/fc/12de89822adaa3a60b8cb0139bae75918278999d08e6dff158623abd7cba/ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 29.4MB/s \n","\u001b[?25hCollecting pytz==2019.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n","\u001b[K     |████████████████████████████████| 512kB 33.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n","Collecting pyopenssl==19.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n","\u001b[?25hCollecting sacremoses==0.0.35\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 27.8MB/s \n","\u001b[?25hCollecting rusenttokenize==0.0.5\n","  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n","Collecting pandas==0.25.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n","\u001b[K     |████████████████████████████████| 10.4MB 30.8MB/s \n","\u001b[?25hCollecting aio-pika==6.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n","\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n","Collecting pytelegrambotapi==3.6.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.5MB/s \n","\u001b[?25hCollecting fastapi==0.47.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hCollecting prometheus-client==0.7.1\n","  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n","Collecting uvloop==0.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/7a/54a80c03b555af21680a2f3692947b43a0d576d90c4c18cace0fee1ccc0e/uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 28.4MB/s \n","\u001b[?25hCollecting Cython==0.29.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/58/2deb24de3c10cc4c0f09639b46f4f4b50059f0fdc785128a57dd9fdce026/Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 36.3MB/s \n","\u001b[?25hCollecting numpy==1.18.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/53/127cb49435bcf5d841baf8eafa030931c62a9eac577a641f8c2293d23371/numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n","\u001b[K     |████████████████████████████████| 20.1MB 1.2MB/s \n","\u001b[?25hCollecting requests==2.22.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n","\u001b[?25hCollecting overrides==2.7.0\n","  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n","Collecting h5py==2.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 25.3MB/s \n","\u001b[?25hCollecting pydantic==1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/56/1f652c3f658d2a9fd495d2e988a2da57eabdb6c4b8f4563c2ccbe6a2a8c5/pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3MB)\n","\u001b[K     |████████████████████████████████| 7.3MB 10.2MB/s \n","\u001b[?25hCollecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 30.2MB/s \n","\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n","Collecting pymorphy2==0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2->deeppavlov) (1.0.1)\n","Collecting websockets==8.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n","\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/2e/485131e3aa113929b425f83854fafc190aa7df716cbeb258c875752f0c6e/httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219kB)\n","\u001b[K     |████████████████████████████████| 225kB 42.9MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.15.0)\n","Collecting cryptography>=2.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 28.1MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n","Collecting yarl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 42.7MB/s \n","\u001b[?25hCollecting aiormq<4,>=3.2.0\n","  Downloading https://files.pythonhosted.org/packages/0b/c4/dc5b9d50c15af2ee187974a5a0c3f20c06cce6559eea4c065d372e846b6a/aiormq-3.3.1-py3-none-any.whl\n","Collecting starlette<=0.12.9,>=0.12.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2021.5.30)\n","Collecting idna<2.9,>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n","\u001b[?25hCollecting dawg-python>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n","Collecting pymorphy2-dicts<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 15.0MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.5)\n","Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n","Collecting multidict>=4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 38.4MB/s \n","\u001b[?25hCollecting pamqp==2.3.0\n","  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n","Building wheels for collected packages: sacremoses, pytelegrambotapi, prometheus-client, overrides, nltk, starlette\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883990 sha256=66915a1c92b3ec44641c40e0c1293ba1524e4b1c0f45c8baa7ce18871a2c3272\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp37-none-any.whl size=47177 sha256=46bad9e0417e7661e5cd795d6dd8173d1565f2424e50e4db3f33f6dd2a8fb934\n","  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n","  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp37-none-any.whl size=41404 sha256=aa99eca2ac2f2ffd8d3c8a7293c7e1ebae7d16152e6c8474e9e983ebf37eca53\n","  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.7.0-cp37-none-any.whl size=5606 sha256=40d402ad5dd5459a8088407f9924cf30ff6b608c710cdc8d0c73e0c73e1b6cea\n","  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449920 sha256=9372d2661497bbeb3551ca4b680083ca0dd2c4af81dac39fe612e9fcd9585b66\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for starlette: filename=starlette-0.12.9-cp37-none-any.whl size=57254 sha256=940ea8d645e1a600d9054269b994caa8ef324ea47b41f6bdf365790f018a67db\n","  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n","Successfully built sacremoses pytelegrambotapi prometheus-client overrides nltk starlette\n","\u001b[31mERROR: xarray 0.18.2 has requirement pandas>=1.0, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: pymorphy2-dicts-ru, numpy, scikit-learn, websockets, uvloop, h11, httptools, uvicorn, ruamel.yaml, pytz, cryptography, pyopenssl, sacremoses, rusenttokenize, pandas, idna, multidict, yarl, pamqp, aiormq, aio-pika, requests, pytelegrambotapi, pydantic, starlette, fastapi, prometheus-client, Cython, overrides, h5py, nltk, dawg-python, pymorphy2-dicts, pymorphy2, deeppavlov\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","  Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Found existing installation: prometheus-client 0.11.0\n","    Uninstalling prometheus-client-0.11.0:\n","      Successfully uninstalled prometheus-client-0.11.0\n","  Found existing installation: Cython 0.29.23\n","    Uninstalling Cython-0.29.23:\n","      Successfully uninstalled Cython-0.29.23\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.4.7 dawg-python-0.7.2 deeppavlov-0.15.0 fastapi-0.47.1 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 multidict-5.1.0 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","pandas","pytz"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"MMk_E0sFm2Di"},"source":["Устанавливаем русскоязычную языковую модель  morpho_ru_syntagrus_pymorphy"]},{"cell_type":"code","metadata":{"id":"zewOSJdunMNC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624602150923,"user_tz":-180,"elapsed":65055,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"ce777324-fe40-4622-bc5a-dd75b8024fa1"},"source":["!python -m deeppavlov install morpho_ru_syntagrus_pymorphy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-25 06:21:10.918 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'morpho_ru_syntagrus_pymorphy' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/morpho_tagger/UD2.0/morpho_ru_syntagrus_pymorphy.json'\n","Collecting tensorflow==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 78kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 33.4MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 34.9MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (57.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=0cca9666556fd7eec80639f0a64f17b1171a3c18dd7c29a864293c46825cce2d\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, gast, keras-applications, tensorflow-estimator, tensorflow\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n","Collecting russian-tagsets==0.6\n","  Downloading https://files.pythonhosted.org/packages/2d/b1/c9377d472a04fb9b84f59365560d68b5d868b589691f32545eb606b3be48/russian-tagsets-0.6.tar.gz\n","Building wheels for collected packages: russian-tagsets\n","  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-cp37-none-any.whl size=24635 sha256=c148cf5d41ecad3be82e2f257f70294540bd5abf6619c3f00251e147e1086985\n","  Stored in directory: /root/.cache/pip/wheels/e8/9d/dd/4679aca4031fdb0d3ad65e165ba5343e61441ed7ad587a08e6\n","Successfully built russian-tagsets\n","Installing collected packages: russian-tagsets\n","Successfully installed russian-tagsets-0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rZU7POPsnkCv"},"source":["Импортируем deeppavlov и инициализируем русскоязычную модель морфологической маркировки"]},{"cell_type":"code","metadata":{"id":"Us5zrG6enqZj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624602178084,"user_tz":-180,"elapsed":27210,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"995b0f20-8ef6-420d-ecee-26e8df521162"},"source":["from deeppavlov import build_model, configs\n","model = build_model(configs.morpho_tagger.UD2_0.morpho_ru_syntagrus_pymorphy, download=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-25 06:22:15.464 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/datasets/UD2.0_source/ru_syntagrus.tar.gz to /root/.deeppavlov/downloads/UD2.0_source/ru_syntagrus.tar.gz\n","100%|██████████| 18.0M/18.0M [00:06<00:00, 2.98MB/s]\n","2021-06-25 06:22:22.828 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/UD2.0_source/ru_syntagrus.tar.gz archive into /root/.deeppavlov/downloads/UD2.0_source/ru_syntagrus\n","2021-06-25 06:22:25.111 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/morpho_tagger/UD2.0/ru_syntagrus.tar.gz to /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus.tar.gz\n","100%|██████████| 30.7M/30.7M [00:06<00:00, 4.40MB/s]\n","2021-06-25 06:22:33.398 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus.tar.gz archive into /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package perluniprops to /root/nltk_data...\n","[nltk_data]   Unzipping misc/perluniprops.zip.\n","[nltk_data] Downloading package nonbreaking_prefixes to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n","2021-06-25 06:22:35.947 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus/tag.dict]\n","2021-06-25 06:22:35.953 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus/char.dict]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_backend.py:38: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:22:37.431 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['morpho_tagger'] at line 166: 99 symbols, 711 tags in CharacterTagger\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:22:39.272 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 165: Model: \"model\"\n","2021-06-25 06:22:39.274 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 166: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.277 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: Layer (type)                    Output Shape         Param #     Connected to                     \n","2021-06-25 06:22:39.279 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 168: ==================================================================================================\n","2021-06-25 06:22:39.283 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: input_1 (InputLayer)            [(None, None, 32)]   0                                            \n","2021-06-25 06:22:39.284 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.286 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: lambda (Lambda)                 (None, None, 32, 99) 0           input_1[0][0]                    \n","2021-06-25 06:22:39.288 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.290 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: dense (Dense)                   (None, None, 32, 32) 3168        lambda[0][0]                     \n","2021-06-25 06:22:39.292 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.294 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d (Conv2D)                 (None, None, 32, 50) 1650        dense[0][0]                      \n","2021-06-25 06:22:39.295 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.297 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_1 (Conv2D)               (None, None, 32, 100 6500        dense[0][0]                      \n","2021-06-25 06:22:39.298 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.300 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_2 (Conv2D)               (None, None, 32, 150 14550       dense[0][0]                      \n","2021-06-25 06:22:39.302 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.303 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_3 (Conv2D)               (None, None, 32, 200 25800       dense[0][0]                      \n","2021-06-25 06:22:39.305 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.307 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_4 (Conv2D)               (None, None, 32, 200 32200       dense[0][0]                      \n","2021-06-25 06:22:39.308 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.310 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_5 (Conv2D)               (None, None, 32, 200 38600       dense[0][0]                      \n","2021-06-25 06:22:39.311 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.313 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: conv2d_6 (Conv2D)               (None, None, 32, 200 45000       dense[0][0]                      \n","2021-06-25 06:22:39.315 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.316 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: concatenate (Concatenate)       (None, None, 32, 110 0           conv2d[0][0]                     \n","2021-06-25 06:22:39.318 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_1[0][0]                   \n","2021-06-25 06:22:39.319 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_2[0][0]                   \n","2021-06-25 06:22:39.321 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_3[0][0]                   \n","2021-06-25 06:22:39.322 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_4[0][0]                   \n","2021-06-25 06:22:39.324 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_5[0][0]                   \n","2021-06-25 06:22:39.325 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  conv2d_6[0][0]                   \n","2021-06-25 06:22:39.326 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.328 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: lambda_1 (Lambda)               (None, None, 1100)   0           concatenate[0][0]                \n","2021-06-25 06:22:39.330 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.332 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: input_2 (InputLayer)            [(None, None, 724)]  0                                            \n","2021-06-25 06:22:39.333 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.335 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: highway (Highway)               (None, None, 1100)   2422200     lambda_1[0][0]                   \n","2021-06-25 06:22:39.336 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.338 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: dense_1 (Dense)                 (None, None, 128)    92800       input_2[0][0]                    \n","2021-06-25 06:22:39.339 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.341 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: concatenate_1 (Concatenate)     (None, None, 1228)   0           highway[0][0]                    \n","2021-06-25 06:22:39.342 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163:                                                                  dense_1[0][0]                    \n","2021-06-25 06:22:39.343 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.345 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: dropout (Dropout)               (None, None, 1228)   0           concatenate_1[0][0]              \n","2021-06-25 06:22:39.346 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.349 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: bidirectional (Bidirectional)   (None, None, 256)    1389568     dropout[0][0]                    \n","2021-06-25 06:22:39.350 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 232: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.352 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 163: p (TimeDistributed)             (None, None, 711)    182727      bidirectional[0][0]              \n","2021-06-25 06:22:39.353 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 230: ==================================================================================================\n","2021-06-25 06:22:39.360 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 242: Total params: 4,254,763\n","2021-06-25 06:22:39.362 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 243: Trainable params: 4,254,763\n","2021-06-25 06:22:39.363 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 244: Non-trainable params: 0\n","2021-06-25 06:22:39.365 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['layer_utils'] at line 245: __________________________________________________________________________________________________\n","2021-06-25 06:22:39.366 INFO in 'deeppavlov.models.morpho_tagger.morpho_tagger'['morpho_tagger'] at line 139: [loading model from /root/.deeppavlov/models/morpho_tagger/UD2.0/ru_syntagrus/model_pymorphy.hdf5]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1gAUg2UboI16"},"source":["Подключаемся к google диску, для загрузки фаила с фрагментом учебного текста.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuhwF-Jd-A0g","executionInfo":{"status":"ok","timestamp":1623934202779,"user_tz":-180,"elapsed":262,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"6c1bf33d-adff-4da1-d4f9-a245d07e855a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R_RT1i55eCaF"},"source":["Открываем и считываем файл"]},{"cell_type":"code","metadata":{"id":"g4p6xEhFeFmt"},"source":["#f = open('/content/drive/MyDrive/Colab Notebooks/test_text.txt', 'r')\n","sentences = [\"я успешно сейчас защищаю диплом\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZE5Db3jGo2VH"},"source":["Производём морфологическую маркировку первого предложения и выведем результат"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PyAH-jehoa1J","executionInfo":{"status":"ok","timestamp":1624603468742,"user_tz":-180,"elapsed":1002,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"f264ea4c-a0a4-42b4-f432-bbc796273270"},"source":["for parse in model(sentences[:1]):\n","    print(parse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\tя\tPRON\tCase=Nom|Number=Sing|Person=1\n","2\tуспешно\tADV\tDegree=Pos\n","3\tсейчас\tADV\tDegree=Pos\n","4\tзащищаю\tVERB\tAspect=Imp|Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin|Voice=Act\n","5\tдиплом\tNOUN\tAnimacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p1aCx7P7tBOT"},"source":["Для русского языка можно использовать модель на основе BERT. Он имеет гораздо более высокую производительность (97,8% вместо 96,2), однако для его обучения понадобится более мощный графический процессор (в идеале 16 ГБ). Однако скорость вывода и обучения на таком графическом процессоре сравнима с символьной моделью.\n","\n","Исключительно для русского языка можно получить лемматизированный вывод UD, используя модель BERT расширенную версию модели Pymorphy. "]},{"cell_type":"code","metadata":{"id":"RMpY8WYIsJNg"},"source":["!python -m deeppavlov install squad_bert\n","model = build_model(configs.morpho_tagger.BERT.morpho_ru_syntagrus_bert, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBgJ4CwKwpV0","executionInfo":{"status":"ok","timestamp":1623892704838,"user_tz":-180,"elapsed":390,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"d2dc831b-8bcf-43cd-a7d4-8ebb1aa635b9"},"source":["for parse in model(sentences[:1]):\n","    print(parse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\tНа\tна\tADP\t_\t_\t_\t_\t_\t_\n","2\tпротяжении\tпротяжение\tNOUN\t_\tAnimacy=Inan|Case=Loc|Gender=Neut|Number=Sing\t_\t_\t_\t_\n","3\tпоследующих\tпоследующий\tADJ\t_\tCase=Gen|Degree=Pos|Number=Plur\t_\t_\t_\t_\n","4\tлет\tгод\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Masc|Number=Plur\t_\t_\t_\t_\n","5\tрусские\tрусский\tADJ\t_\tCase=Nom|Degree=Pos|Number=Plur\t_\t_\t_\t_\n","6\tземли\tземля\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Fem|Number=Plur\t_\t_\t_\t_\n","7\tокажутся\tоказаться\tVERB\t_\tAspect=Perf|Mood=Ind|Number=Plur|Person=3|Tense=Fut|VerbForm=Fin|Voice=Mid\t_\t_\t_\t_\n","8\tв\tв\tADP\t_\t_\t_\t_\t_\t_\n","9\tогне\tогонь\tNOUN\t_\tAnimacy=Inan|Case=Loc|Gender=Masc|Number=Sing\t_\t_\t_\t_\n","10\tмежкняжеского\tмежкняжеский\tADJ\t_\tCase=Gen|Degree=Pos|Gender=Neut|Number=Sing\t_\t_\t_\t_\n","11\tпротивостояния\tпротивостояние\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Neut|Number=Sing\t_\t_\t_\t_\n","12\t.\t.\tPUNCT\t_\t_\t_\t_\t_\t_\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"voab-Z-Aw7Zm"},"source":["Морфологическая маркировка заключается в присвоении меток, описывающих морфологию слова, предварительно размеченной последовательности слов. В самом простом случае эти метки являются просто тегами части речи (POS), поэтому в ранние времена NLP задача часто называлась POS-тегированием. Уточненная версия проблемы, которую мы здесь решаем, выполняет более тонкую классификацию, также обнаруживая значения других морфологических признаков, таких как падеж, род и число для существительных, наклонение, время для глаголов и т. д. Морфологическая маркировка - это этап общего конвейера NLP, он генерирует полезные функции для дальнейших задач, таких как синтаксический анализ, распознавание именованных сущностей или машинный перевод."]},{"cell_type":"markdown","metadata":{"id":"0r4ZxafryEGc"},"source":["Для морфологической маркировки  применяется нейронная модель от Heigold et al., 2017(Обширная эмпирическая оценка символьной морфологической маркировки для 14 языков). Устройство тегов состоит из двух частей: сети на уровне символов, которая создает embeddings для отдельных слов, и рекуррентной сети на уровне слов, которая преобразует эти embeddings в морфологические теги.\n","\n","Часть символьного уровня реализует модель из Kim et al., 2015(Символьные языковые модели). Сначала он встраивает символы в плотные векторы, затем пропускает эти векторы через несколько параллельных сверточных слоев и объединяет выходные данные этих сверток. Выходные данные свертки распространяются через слой шоссе, чтобы получить окончательное представление слова.\n","\n","Можно дополнительно использовать морфологический словарь во время тегирования. В этом случае наша модель собирает вектор 0/1 с векторами, соответствующими словарным тегам текущего слова. Этот вектор проходит через однослойный перцептрон, чтобы получить встраивание словарной информации. Это вложение объединяется с выводом сети на уровне символов.\n","\n","В качестве сети на уровне слов мы используем двунаправленный LSTM, его выходы проецируются через плотный слой с активацией softmax. В принципе, несколько слоев BiLSTM могут быть наложены друг на друга, а также несколько сверточных слоев или слоев шоссе на уровне персонажа; однако мы не наблюдали достаточного прироста производительности и поэтому использовали неглубокую архитектуру."]},{"cell_type":"markdown","metadata":{"id":"ljF1tQOx1GV-"},"source":["# **Распознавание именованных сущностей**"]},{"cell_type":"markdown","metadata":{"id":"MJ8e8UYW17vF"},"source":["Доступны три основных типа моделей: стандартная модель на основе RNN, модель на основе BERT (на TensorFlow и PyTorch) и гибридная модель. Любая предварительно обученная модель может использоваться для вывода как из интерфейса командной строки (CLI), так и из Python."]},{"cell_type":"markdown","metadata":{"id":"TWTZzlER2uIA"},"source":["Установим все необходимые пакеты для работы с русским языком с помощью команды:"]},{"cell_type":"code","metadata":{"id":"IIdo-I1q21_D"},"source":["!python -m deeppavlov install ner_rus_bert"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5EtQ5t4r3Esp"},"source":["Чтобы использовать предварительно обученную модель используем следующую команду:"]},{"cell_type":"code","metadata":{"id":"gLFFzP333IUq"},"source":["!python -m deeppavlov interact ner_rus_bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0UPYlsZ4HAB"},"source":["from deeppavlov import configs, build_model\n","ner_model = build_model(configs.ner.ner_rus_bert, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YW8_YJfWzRKv","executionInfo":{"status":"ok","timestamp":1623896942247,"user_tz":-180,"elapsed":413,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"9bd93b1e-5036-40c1-eeac-8e315e9f9a31"},"source":["print(ner_model(sentences[3:4]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[['Так', ',', 'Святополк', 'Изяславич', 'получил', 'Киев', 'с', 'Туровом', 'и', 'Пинском', 'и', 'титул', 'великого', 'князя', '.', '\\n']], [['O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gcgNGjTDzbUi"},"source":["# Синтаксический анализ"]},{"cell_type":"markdown","metadata":{"id":"oWK4fjqrzfoG"},"source":["Синтаксический анализ - это задача прогнозирования синтаксического дерева с учетом токенизированного (или необработанного) предложения. Типичный вывод парсера выглядит так:\n","Чтобы определить дерево, для каждого слова нужно знать его синтаксический заголовок и метку зависимости для грани между ними.\n"]},{"cell_type":"markdown","metadata":{"id":"ikKZiUcS0GWs"},"source":["Синтаксические деревья можно использовать во многих задачах извлечения информации. Например, чтобы определить, кто победитель, а кто проигравший, в предложении «Манчестер победил Ливерпуль» полагаются на порядок слов. Однако во многих языках, таких как русский, испанский и немецкий, порядок слов относительно свободный, а это значит, что нам нужны другие подсказки. Также обратите внимание, что синтаксические отношения (nsubj, obj и т. Д.) Имеют четкие семантические аналоги, что делает синтаксический анализ привлекательным шагом предварительной обработки для семантически-ориентированных задач."]},{"cell_type":"markdown","metadata":{"id":"CjLw6Lax0eBi"},"source":["Устанавливаем русскоязычную модель для синтаксического анализа"]},{"cell_type":"code","metadata":{"id":"XApeTgCi2HrF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623932129021,"user_tz":-180,"elapsed":78836,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"50075728-7309-4f78-ac2f-c02cc00abcba"},"source":["!python -m deeppavlov install syntax_ru_syntagrus_bert"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-17 12:14:12.499 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'syntax_ru_syntagrus_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/syntax/syntax_ru_syntagrus_bert.json'\n","Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n","  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-0ztlz6m3\n","  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-0ztlz6m3\n","Building wheels for collected packages: bert-dp\n","  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23593 sha256=20550c133d4ba5d1299fba1176cec7a07ce0599501ee9faab0f0fb350a612295\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ml74534x/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n","Successfully built bert-dp\n","Installing collected packages: bert-dp\n","Successfully installed bert-dp-1.0\n","Collecting git+https://github.com/andersjo/dependency_decoding.git@79510908223b93bd4c1fb0409a2a66dd75577c2c\n","  Cloning https://github.com/andersjo/dependency_decoding.git (to revision 79510908223b93bd4c1fb0409a2a66dd75577c2c) to /tmp/pip-req-build-5yp3vt1f\n","  Running command git clone -q https://github.com/andersjo/dependency_decoding.git /tmp/pip-req-build-5yp3vt1f\n","Building wheels for collected packages: dependency-decoding\n","  Building wheel for dependency-decoding (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dependency-decoding: filename=dependency_decoding-0.0.1-cp37-cp37m-linux_x86_64.whl size=367444 sha256=8176558bd645b2dfec96dedc0e383cb030739dc153d613e2bdfb1d8ed5c395b4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xnhtc9ff/wheels/a3/30/16/d1097a4f151d6e560cf3c54e50bf3d28eeafe3dfeccd6c73e1\n","Successfully built dependency-decoding\n","Installing collected packages: dependency-decoding\n","Successfully installed dependency-decoding-0.0.1\n","Collecting tensorflow==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 82kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 30.0MB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 35.5MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=1b9933926592a303d98f68c1678427ad0d5aa8d96f157af1e7302ac49101c5e0\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, tensorflow-estimator, gast, keras-applications, tensorflow\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GBxBUHoz2zei"},"source":["Эта модель производит вывод в формате CONLL-U и обучается на корпусах универсальных зависимостей."]},{"cell_type":"markdown","metadata":{"id":"XwnWZsSG5dRu"},"source":["Импортируем модель"]},{"cell_type":"code","metadata":{"id":"tASKUoqm3FiV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623934196327,"user_tz":-180,"elapsed":32770,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"dcf169ec-bfe7-4235-ee81-78224c10f36c"},"source":["from deeppavlov import build_model, configs\n","model = build_model(configs.syntax.syntax_ru_syntagrus_bert, download=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-17 12:49:26.72 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/morpho_tagger/UD2.3/ru_syntagrus.tar.gz download because of matching hashes\n","2021-06-17 12:49:28.556 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz download because of matching hashes\n","2021-06-17 12:49:31.59 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/syntax_parser/syntax_ru_syntagrus_bert.tar.gz download because of matching hashes\n","2021-06-17 12:49:31.507 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/syntax_ru_syntagrus/deps.dict]\n","2021-06-17 12:49:55.620 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/syntax_ru_syntagrus/model_joint]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/syntax_ru_syntagrus/model_joint\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Vtolo785jEn"},"source":["Запускаем синтаксический анализ"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X29L-Xi75W5T","executionInfo":{"status":"ok","timestamp":1623934212473,"user_tz":-180,"elapsed":3013,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"650a152a-d7dd-41ba-a78e-190807c224e7"},"source":["syntaxRes = model(sentences[:1])\n","for parse in syntaxRes:\n","    print(parse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\tНа\t_\t_\t_\t_\t2\tcase\t_\t_\n","2\tпротяжении\t_\t_\t_\t_\t7\tobl\t_\t_\n","3\tпоследующих\t_\t_\t_\t_\t4\tamod\t_\t_\n","4\tлет\t_\t_\t_\t_\t2\tnmod\t_\t_\n","5\tрусские\t_\t_\t_\t_\t6\tamod\t_\t_\n","6\tземли\t_\t_\t_\t_\t7\tnsubj\t_\t_\n","7\tокажутся\t_\t_\t_\t_\t0\troot\t_\t_\n","8\tв\t_\t_\t_\t_\t9\tcase\t_\t_\n","9\tогне\t_\t_\t_\t_\t7\tobl\t_\t_\n","10\tмежкняжеского\t_\t_\t_\t_\t11\tamod\t_\t_\n","11\tпротивостояния\t_\t_\t_\t_\t9\tnmod\t_\t_\n","12\t.\t_\t_\t_\t_\t7\tpunct\t_\t_\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTNYd-Xy-7XB"},"source":["#f = open(\"/content/syntaxRes.conllu\", \"a\")\n","#for parse in syntaxRes:\n","#    f.write(parse)\n","#f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWj7hZSUQPz0"},"source":["Существует модель, которая поддерживает совместное предсказание морфологических тегов и синтаксической информации, однако качество совместной модели немного уступает отдельным. Поэтому подключаем специальный компонент, который может объединять выходные данные морфологического и синтаксического анализа: JointTaggerParser. Его пример вывода для русского языка с настройками по умолчанию выглядит так:"]},{"cell_type":"code","metadata":{"id":"0tPuT8amQQYF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624604341920,"user_tz":-180,"elapsed":732750,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"c21ffb74-4657-413c-9e82-b197ee32580b"},"source":["!python -m deeppavlov install ru_syntagrus_joint_parsing\n","from deeppavlov import build_model, configs\n","model = build_model(\"ru_syntagrus_joint_parsing\", download=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-25 06:46:50.216 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ru_syntagrus_joint_parsing' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/syntax/ru_syntagrus_joint_parsing.json'\n","Requirement already satisfied: russian-tagsets==0.6 in /usr/local/lib/python3.7/dist-packages (0.6)\n","Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n","  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-mhof1bjb\n","  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-mhof1bjb\n","Building wheels for collected packages: bert-dp\n","  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23593 sha256=0a71550184cc5478faba59af8c066063e9fd8e9c2116ec57ae889649cb2f586b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hyoac5we/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n","Successfully built bert-dp\n","Installing collected packages: bert-dp\n","Successfully installed bert-dp-1.0\n","Collecting git+https://github.com/andersjo/dependency_decoding.git@79510908223b93bd4c1fb0409a2a66dd75577c2c\n","  Cloning https://github.com/andersjo/dependency_decoding.git (to revision 79510908223b93bd4c1fb0409a2a66dd75577c2c) to /tmp/pip-req-build-n_q25hvd\n","  Running command git clone -q https://github.com/andersjo/dependency_decoding.git /tmp/pip-req-build-n_q25hvd\n","Building wheels for collected packages: dependency-decoding\n","  Building wheel for dependency-decoding (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dependency-decoding: filename=dependency_decoding-0.0.1-cp37-cp37m-linux_x86_64.whl size=367453 sha256=b6c777b176079132c32443ef08c5fba566fc0954bdc1a27c1c72eb28d3e4337f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6_1pjcrp/wheels/a3/30/16/d1097a4f151d6e560cf3c54e50bf3d28eeafe3dfeccd6c73e1\n","Successfully built dependency-decoding\n","Installing collected packages: dependency-decoding\n","Successfully installed dependency-decoding-0.0.1\n","Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (57.0.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:47:14.503 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ru_syntagrus_joint_parsing' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/syntax/ru_syntagrus_joint_parsing.json'\n","2021-06-25 06:47:16.60 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/morpho_tagger/UD2.3/ru_syntagrus.tar.gz to /root/.deeppavlov/downloads/UD2.3_source/ru_syntagrus.tar.gz\n","100%|██████████| 16.7M/16.7M [00:07<00:00, 2.23MB/s]\n","2021-06-25 06:47:24.884 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/UD2.3_source/ru_syntagrus.tar.gz archive into /root/.deeppavlov/downloads/UD2.3_source/ru_syntagrus\n","2021-06-25 06:47:27.53 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/syntax_parser/syntax_ru_syntagrus_bert.tar.gz to /root/.deeppavlov/models/syntax_ru_syntagrus_bert.tar.gz\n","100%|██████████| 693M/693M [03:25<00:00, 3.37MB/s]\n","2021-06-25 06:50:53.978 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/models/syntax_ru_syntagrus_bert.tar.gz archive into /root/.deeppavlov/models/syntax_ru_syntagrus\n","2021-06-25 06:51:03.655 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/morpho_tagger/BERT/morpho_ru_syntagrus_bert.tar.gz to /root/.deeppavlov/models/morpho_ru_syntagrus_bert.tar.gz\n","100%|██████████| 661M/661M [02:57<00:00, 3.72MB/s]\n","2021-06-25 06:54:02.531 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/models/morpho_ru_syntagrus_bert.tar.gz archive into /root/.deeppavlov/models/morpho_ru_syntagrus\n","2021-06-25 06:54:11.683 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz to /root/.deeppavlov/downloads/rubert_cased_L-12_H-768_A-12_v1.tar.gz\n","100%|██████████| 666M/666M [03:25<00:00, 3.25MB/s]\n","2021-06-25 06:57:38.65 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/rubert_cased_L-12_H-768_A-12_v1.tar.gz archive into /root/.deeppavlov/downloads/bert_models\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:57:46.501 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/morpho_ru_syntagrus/tag.dict]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n","\n","WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:571: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:58:08.933 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/morpho_ru_syntagrus/model]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/morpho_ru_syntagrus/model\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:58:10.781 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n","2021-06-25 06:58:11.242 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/syntax_ru_syntagrus/deps.dict]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/layers/tf_layers.py:161: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/layers/tf_layers.py:181: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/layers/tf_layers.py:185: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2021-06-25 06:58:39.219 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/syntax_ru_syntagrus/model_joint]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/syntax_ru_syntagrus/model_joint\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vQKcLHCqeZb","executionInfo":{"status":"ok","timestamp":1624609271047,"user_tz":-180,"elapsed":2376,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"1dea336a-e887-45fd-9ea2-d29a0cf48568"},"source":["syntaxRes = model([\"Старченко сегодня сдаёт экзамен\"])\n","for parse in syntaxRes:\n","    print(parse)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["1\tСтарченко\tстарченко\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t3\tnsubj\t_\t_\n","2\tсегодня\tсегодня\tADV\t_\tDegree=Pos\t3\tadvmod\t_\t_\n","3\tсдаёт\tсдавать\tVERB\t_\tAspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n","4\tэкзамен\tэкзамен\tNOUN\t_\tAnimacy=Inan|Case=Acc|Gender=Masc|Number=Sing\t3\tobj\t_\t_\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bCKx2pcV6HHg"},"source":["\n","Для визуализации результата используем CoNLL-U viewer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"id":"XJiJvdDl_zlZ","executionInfo":{"status":"ok","timestamp":1624609280850,"user_tz":-180,"elapsed":1361,"user":{"displayName":"Starchenko Nikita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8KM-3QRSTS9qfAkgvoZPER_2-Ph-_iChrFRArcA=s64","userId":"02210374188233191641"}},"outputId":"4569b558-7f29-4814-cba4-cfd4276cda04"},"source":["import requests\n","import IPython\n","from IPython.display import display,Javascript\n","req=requests.get('https://urd2.let.rug.nl/~kleiweg/conllu/bin/form', files = {'conllu': syntaxRes[0]})\n","IPython.display.HTML(req.text)\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<!DOCTYPE html>\n","<html>\n","<head>\n","<meta charset=\"utf-8\">\n","<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n","<meta name=\"robots\" content=\"noindex,nofollow\">\n","<title>Universal Dependencies</title>\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"../../conllu/tooltip.css\" />\n","<script type=\"text/javascript\" src=\"../../conllu/jquery.js\"></script>\n","<script type=\"text/javascript\" src=\"../../conllu/tooltip.js\"></script>\n","<script type=\"text/javascript\">\n","var tts = [];\n","var normal = [];\n","function toggle(id, enhanced) {\n","  unhold();\n","  if (normal[id] && enhanced) {\n","    $('svg#'+id+' .enhanced').css({'visibility':''});\n","    $('svg#'+id+' .normal').css({'visibility':'hidden'});\n","    normal[id] = false;\n","  } else if (!normal[id] && !enhanced) {\n","    $('svg#'+id+' .enhanced').css({'visibility':'hidden'});\n","    $('svg#'+id+' .normal').css({'visibility':''});\n","    normal[id] = true;\n","  }\n","}\n","function toggleAll(enhanced) {\n","  unhold();\n","  if (enhanced) {\n","    $('.enhanced').css({'visibility':''});\n","    $('.normal').css({'visibility':'hidden'});\n","    $('.rb').prop('checked', false);\n","    $('.re').prop('checked', true);\n","  } else {\n","    $('.enhanced').css({'visibility':'hidden'});\n","    $('.normal').css({'visibility':''});\n","    $('.re').prop('checked', false);\n","    $('.rb').prop('checked', true);\n","  }\n","  for (var id in tts) {\n","    normal[id] = !enhanced;\n","  }\n","}\n","\n","var holding  = false;\n","var holdedge = false;\n","var holdnode = false;\n","var holdid  = '';\n","var holdi = -1;\n","var holdj = -1;\n","\n","function unhold() {\n","  if (holding) {\n","    holding = false;\n","    if (holdnode) {\n","      holdnode = false;\n","      unmark(holdid, holdi);\n","    }\n","    if (holdedge) {\n","      holdedge = false;\n","      unmrk(holdid, holdi, holdj);\n","    }\n","    holdid = '';\n","    holdi = -1;\n","    holdj = -1;\n","  }\n","}\n","function hold(id, i) {\n","  if (holdnode && id == holdid && i == holdi) {\n","    unhold();\n","  } else {\n","    unhold();\n","    mark(id, i);\n","    holding = true;\n","    holdnode = true;\n","    holdid = id;\n","    holdi = i;\n","  }\n","}\n","function hld(id, i, j) {\n","  if (holdedge && id == holdid && i == holdi && j == holdj) {\n","    unhold();\n","  } else {\n","    unhold();\n","    mrk(id, i, j);\n","    holding = true;\n","    holdedge = true;\n","    holdid = id;\n","    holdi = i;\n","    holdj = j;\n","  }\n","}\n","function mark(id, i) {\n","  var cl = normal[id] ? 'n' : 'e';\n","  var t = tts[id][i-1];\n","  tooltip.show('<em>' + t[0] + '</em><br>\\n' + t[1] + '<br>\\n' + t[2] + '<br>\\nLemma: ' + t[3] + (t[4] == \"_\" ? \"\" : '<br>\\nXpos: ' + t[4]));\n","  if (!holding) {\n","    $('svg#' + id + ' .l' + cl + i).css({'fill':'blue','font-weight':'bold'});\n","    $('svg#' + id + ' .e' + cl + i).css({'stroke':'blue','stroke-width':3});\n","  }\n","}\n","function unmark(id, i) {\n","  var cl = normal[id] ? 'n' : 'e';\n","  tooltip.hide();\n","  if (!holding) {\n","    $('svg#' + id + ' .l' + cl + i).css({'fill':'black','font-weight':'normal'});\n","    $('svg#' + id + ' .e' + cl + i).css({'stroke':'black','stroke-width':1});\n","  }\n","}\n","function mrk(id, i, j) {\n","  if (!holding) {\n","    $('svg#' + id + ' .p' + i + 'p' + j).css({'fill':'blue','font-weight':'bold'});\n","    $('svg#' + id + ' .q' + i + 'q' + j).css({'stroke':'blue','stroke-width':3});\n","    $('svg#' + id + ' .q' + i).css({'stroke':'blue','stroke-width':3});\n","    $('svg#' + id + ' .q' + j).css({'stroke':'blue','stroke-width':3});\n","  }\n","}\n","function unmrk(id, i, j) {\n","  if (!holding) {\n","    $('svg#' + id + ' .p' + i + 'p' + j).css({'fill':'black','font-weight':'normal'});\n","    $('svg#' + id + ' .q' + i + 'q' + j).css({'stroke':'black','stroke-width':1});\n","    $('svg#' + id + ' .q' + i).css({'stroke':'black','stroke-width':1});\n","    $('svg#' + id + ' .q' + j).css({'stroke':'black','stroke-width':1});\n","  }\n","}\n","</script>\n","<style type=\"text/css\">\n","  body { padding-bottom: 200px; }\n","  #top {\n","    margin-bottom: 2em;\n","  }\n","  div.comments {\n","    padding: 1em;\n","    margin: 2em 0px 1em 0px;\n","    background-color: #f0f0ff;\n","    white-space: pre-line;\n","  }\n","  div.svg {\n","    margin: 40px 0px;\n","  }\n","  div.overflow {\n","    overflow-x: auto;\n","  }\n","  .udcontrol input,\n","  .udcontrol label {\n","    cursor: pointer;\n","  }\n","  .udcontrol label:hover {\n","    color: #0000e0;\n","    text-decoration: underline;\n","  }\n","</style>\n","</head>\n","<body>\n","<div id=\"top\">\n","Select all:\n","<button onclick=\"toggleAll(false)\">Basic</button>\n","<button onclick=\"toggleAll(true)\">Enhanced</button>\n","</div>\n","<div class=\"overflow\">\n","<svg id=\"svg1\" width=\"452\" height=\"160\">\n","<g class=\"normal\" style=\"visibility:hidden\">\n","<g fill=\"none\" stroke=\"black\" stroke-width=\"1\">\n","<path class=\"en2 en3 q2q3\" d=\"M193 108 L193 148 C193 68 193 68 222 68 C252 68 252 68 252 148 L252 108\" />\n","<path class=\"en4 en3 q4q3\" d=\"M375 108 L375 148 C375 68 375 68 345 68 C315 68 315 68 315 148 L315 108\" />\n","<path class=\"en1 en3 q1q3\" d=\"M80 108 L80 108 C80 28 80 28 176 28 C273 28 273 28 273 108 L273 108\" />\n","<path class=\"en3 q3q0\" d=\"M294 108 L294 28\" />\n","</g>\n","<g fill=\"black\" stroke-width=\"1\" stroke=\"black\">\n","<path class=\"en2 en3 q2q3\" d=\"M193 108 l3 -14 l-6 0 Z\" />\n","<path class=\"en4 en3 q4q3\" d=\"M375 108 l3 -14 l-6 0 Z\" />\n","<path class=\"en1 en3 q1q3\" d=\"M80 108 l3 -14 l-6 0 Z\" />\n","<path class=\"en3 en0 q3q0\" d=\"M294 108 l3 -14 l-6 0 Z\" />\n","</g>\n","<g fill=\"white\" stroke=\"white\" stroke-width=\"1\" opacity=\"0.9\">\n","<rect x=\"187\" y=\"47\" width=\"70\" height=\"19\" />\n","<rect x=\"329\" y=\"47\" width=\"32\" height=\"19\" />\n","<rect x=\"151\" y=\"7\" width=\"51\" height=\"19\" />\n","<rect x=\"274\" y=\"7\" width=\"39\" height=\"19\" />\n","</g>\n","<g font-family=\"FreeSans, Arial, Helvetica, sans-serif\" font-size=\"16\" text-anchor=\"middle\">\n","<text class=\"ln2 ln3 p2p3\" x=\"222\" y=\"60\" onclick=\"hld('svg1',2,3)\" onmouseover=\"mrk('svg1',2,3)\" onmouseout=\"unmrk('svg1',2,3)\">advmod</text>\n","<text class=\"ln4 ln3 p4p3\" x=\"345\" y=\"60\" onclick=\"hld('svg1',4,3)\" onmouseover=\"mrk('svg1',4,3)\" onmouseout=\"unmrk('svg1',4,3)\">obj</text>\n","<text class=\"ln1 ln3 p1p3\" x=\"176\" y=\"20\" onclick=\"hld('svg1',1,3)\" onmouseover=\"mrk('svg1',1,3)\" onmouseout=\"unmrk('svg1',1,3)\">nsubj</text>\n","<text class=\"ln3 ln0 p3p0\" x=\"294\" y=\"20\" onclick=\"hld('svg1',3,0)\" onmouseover=\"mrk('svg1',3,0)\" onmouseout=\"unmrk('svg1',3,0)\">root</text>\n","</g>\n","</g>\n","<g fill=\"#D0E0FF\" stroke=\"black\" stroke-width=\"1\">\n","<rect class=\"q1 en1 en3\" x=\"4\" y=\"108\" width=\"108\" height=\"48\" rx=\"5\" ry=\"5\" />\n","<rect class=\"q2 en2 en3\" x=\"120\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" />\n","<rect class=\"q3 en0 en2 en3 en4 en1\" x=\"232\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" />\n","<rect class=\"q4 en4 en3\" x=\"344\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" />\n","</g>\n","<g font-family=\"FreeSans, Arial, Helvetica, sans-serif\" font-size=\"16\" text-anchor=\"middle\">\n","<text x=\"58\" y=\"127\">PROPN</text>\n","<text x=\"172\" y=\"127\">ADV</text>\n","<text x=\"284\" y=\"127\">VERB</text>\n","<text x=\"396\" y=\"127\">NOUN</text>\n","</g>\n","<g font-family=\"FreeSans, Arial, Helvetica, sans-serif\" font-size=\"16\" text-anchor=\"middle\" font-style=\"italic\">\n","<text x=\"58\" y=\"147\">Старченко</text>\n","<text x=\"172\" y=\"147\">сегодня</text>\n","<text x=\"284\" y=\"147\">сдаёт</text>\n","<text x=\"396\" y=\"147\">экзамен</text>\n","</g>\n","<g opacity=\"0\" stroke-width=\"1\">\n","<rect x=\"4\" y=\"108\" width=\"108\" height=\"48\" rx=\"5\" ry=\"5\" onclick=\"hold('svg1',1)\" onmouseover=\"mark('svg1',1)\" onmouseout=\"unmark('svg1',1)\" />\n","<rect x=\"120\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" onclick=\"hold('svg1',2)\" onmouseover=\"mark('svg1',2)\" onmouseout=\"unmark('svg1',2)\" />\n","<rect x=\"232\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" onclick=\"hold('svg1',3)\" onmouseover=\"mark('svg1',3)\" onmouseout=\"unmark('svg1',3)\" />\n","<rect x=\"344\" y=\"108\" width=\"104\" height=\"48\" rx=\"5\" ry=\"5\" onclick=\"hold('svg1',4)\" onmouseover=\"mark('svg1',4)\" onmouseout=\"unmark('svg1',4)\" />\n","</g>\n","<g fill=\"#D0E0FF\" stroke=\"black\" stroke-width=\"1\">\n","</g>\n","<g font-family=\"FreeSans, Arial, Helvetica, sans-serif\" font-size=\"16\" font-style=\"italic\" text-anchor=\"middle\">\n","</g>\n","\n","</svg>\n","</div>\n","<script type=\"text/javascript\">\n","tts['svg1'] = [\n","['Старченко','PROPN','Animacy: Anim, Case: Nom, Gender: Masc, Number: Sing','старченко','_'],\n","['сегодня','ADV','Degree: Pos','сегодня','_'],\n","['сдаёт','VERB','Aspect: Imp, Mood: Ind, Number: Sing, Person: 3, Tense: Pres, VerbForm: Fin, Voice: Act','сдавать','_'],\n","['экзамен','NOUN','Animacy: Inan, Case: Acc, Gender: Masc, Number: Sing','экзамен','_']\n","];toggle('svg1',false);\n","</script>\n","<div class=\"udcontrol\">\n","<input type=\"radio\" id=\"btnbsvg1\" name=\"btnsvg1\" onclick=\"toggle('svg1',false)\" class=\"rb\" checked /><label for=\"btnbsvg1\">Basic</label>\n","</div>\n","</body>\n","</html>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"i2EWWPgUF0dy"},"source":["<iframe src = \"/content/visual.html\"></iframe>"]},{"cell_type":"code","metadata":{"id":"C8V6SIXJB9z3"},"source":[""],"execution_count":null,"outputs":[]}]}